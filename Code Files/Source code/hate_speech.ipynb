{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1615791565470,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "td45jZYHPNMh"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1929,
     "status": "ok",
     "timestamp": 1615791566479,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "lZS_CITdPVDJ"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\loki\\\\new project id\\\\hate speech with web application\\\\twitter_hate_speech_detection-master\\\\src/twitter_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mloki\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mnew project id\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mhate speech with web application\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtwitter_hate_speech_detection-master\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39msrc/twitter_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ANGELINE\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ANGELINE\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ANGELINE\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\ANGELINE\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ANGELINE\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\ANGELINE\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ANGELINE\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\loki\\\\new project id\\\\hate speech with web application\\\\twitter_hate_speech_detection-master\\\\src/twitter_data.csv'"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r\"D:\\loki\\new project id\\hate speech with web application\\twitter_hate_speech_detection-master\\src/twitter_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 1924,
     "status": "ok",
     "timestamp": 1615791566481,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "LXi5NR0tSEcv",
    "outputId": "0b783255-f1dc-4691-e557-b263994e2a56"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1916,
     "status": "ok",
     "timestamp": 1615791566482,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "orW8A18HDzrM",
    "outputId": "4b27bca7-80f7-4e6e-b850-c827f12aaeb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 1909,
     "status": "ok",
     "timestamp": 1615791566483,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "NA-Unn9_D6as",
    "outputId": "5f3c7c60-7000-4030-89a6-f3452177b1a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12681.192027</td>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7299.553863</td>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6372.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12703.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18995.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25296.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
       "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
       "mean   12681.192027      3.243473      0.280515            2.413711   \n",
       "std     7299.553863      0.883060      0.631851            1.399459   \n",
       "min        0.000000      3.000000      0.000000            0.000000   \n",
       "25%     6372.500000      3.000000      0.000000            2.000000   \n",
       "50%    12703.000000      3.000000      0.000000            3.000000   \n",
       "75%    18995.500000      3.000000      0.000000            3.000000   \n",
       "max    25296.000000      9.000000      7.000000            9.000000   \n",
       "\n",
       "            neither         class  \n",
       "count  24783.000000  24783.000000  \n",
       "mean       0.549247      1.110277  \n",
       "std        1.113299      0.462089  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      1.000000  \n",
       "50%        0.000000      1.000000  \n",
       "75%        0.000000      1.000000  \n",
       "max        9.000000      2.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1901,
     "status": "ok",
     "timestamp": 1615791566483,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "70nh4K0zSJuV"
   },
   "outputs": [],
   "source": [
    "dataset['hate_speech'] = dataset['hate_speech'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1897,
     "status": "ok",
     "timestamp": 1615791566484,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "nPKhXGQSSNPn",
    "outputId": "5c1140ff-fb69-46d8-d8ac-cdd56844a692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   Unnamed: 0          24783 non-null  int64   \n",
      " 1   count               24783 non-null  int64   \n",
      " 2   hate_speech         24783 non-null  category\n",
      " 3   offensive_language  24783 non-null  int64   \n",
      " 4   neither             24783 non-null  int64   \n",
      " 5   class               24783 non-null  int64   \n",
      " 6   tweet               24783 non-null  object  \n",
      "dtypes: category(1), int64(5), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2225,
     "status": "ok",
     "timestamp": 1615791566821,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "jyJAq9XgSSV0"
   },
   "outputs": [],
   "source": [
    "#Remove the special characters, numbers etc.\n",
    "#lemmatize the text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2220,
     "status": "ok",
     "timestamp": 1615791566822,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "MuKi8IVgXArz",
    "outputId": "9add9dbe-5a64-4820-8acd-822acc8a6b9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14518,
     "status": "ok",
     "timestamp": 1615791579127,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "NCYWWoWkSnkm"
   },
   "outputs": [],
   "source": [
    "dataset['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]',' ',text)) for text in lis]) for lis in dataset['tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi513_qjawpx"
   },
   "source": [
    "#Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14514,
     "status": "ok",
     "timestamp": 1615791579128,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "gQ06m8GNXDqd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(dataset['text_lem'],dataset['hate_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16948,
     "status": "ok",
     "timestamp": 1615791581568,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "Kyjy442SCAhc"
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range = (1,4)).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18414,
     "status": "ok",
     "timestamp": 1615791583039,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "z0Wl4UoECEHg",
    "outputId": "9d181bfb-0598-4139-bc10-8f20305b43c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 402782)\t0.22133207487639361\n",
      "  (0, 402781)\t0.22133207487639361\n",
      "  (0, 402780)\t0.22133207487639361\n",
      "  (0, 401959)\t0.0853399313743502\n",
      "  (0, 354961)\t0.22133207487639361\n",
      "  (0, 354960)\t0.22133207487639361\n",
      "  (0, 354959)\t0.22133207487639361\n",
      "  (0, 354803)\t0.11142719882129813\n",
      "  (0, 242432)\t0.22133207487639361\n",
      "  (0, 242431)\t0.22133207487639361\n",
      "  (0, 242430)\t0.22133207487639361\n",
      "  (0, 242418)\t0.18849233050367833\n",
      "  (0, 222132)\t0.22133207487639361\n",
      "  (0, 222131)\t0.22133207487639361\n",
      "  (0, 222129)\t0.18046349647910542\n",
      "  (0, 219593)\t0.06983228395519397\n",
      "  (0, 187036)\t0.22133207487639361\n",
      "  (0, 187035)\t0.22133207487639361\n",
      "  (0, 187032)\t0.20619802906794987\n",
      "  (0, 187013)\t0.17592993745106236\n",
      "  (0, 87920)\t0.22133207487639361\n",
      "  (0, 87919)\t0.22133207487639361\n",
      "  (0, 87915)\t0.16450543541309878\n",
      "  (0, 66590)\t0.20132595453289953\n",
      "  (0, 66589)\t0.17105786291601202\n",
      "  :\t:\n",
      "  (18586, 186773)\t0.12600866023694657\n",
      "  (18586, 186772)\t0.12600866023694657\n",
      "  (18586, 186768)\t0.11461878634528096\n",
      "  (18586, 186690)\t0.07586137562652101\n",
      "  (18586, 171419)\t0.12600866023694657\n",
      "  (18586, 171417)\t0.12096856074867976\n",
      "  (18586, 171398)\t0.09154423229803932\n",
      "  (18586, 170757)\t0.049903534890578014\n",
      "  (18586, 149921)\t0.12600866023694657\n",
      "  (18586, 149920)\t0.12600866023694657\n",
      "  (18586, 149919)\t0.11739255325221974\n",
      "  (18586, 149869)\t0.09564927345624608\n",
      "  (18586, 117542)\t0.05557262596432986\n",
      "  (18586, 109728)\t0.12600866023694657\n",
      "  (18586, 109727)\t0.12600866023694657\n",
      "  (18586, 109726)\t0.12600866023694657\n",
      "  (18586, 109719)\t0.10877644626749296\n",
      "  (18586, 36908)\t0.12600866023694657\n",
      "  (18586, 36907)\t0.12600866023694657\n",
      "  (18586, 36904)\t0.12096856074867976\n",
      "  (18586, 36469)\t0.05957152706567313\n",
      "  (18586, 5610)\t0.12600866023694657\n",
      "  (18586, 5608)\t0.11739255325221974\n",
      "  (18586, 5604)\t0.0900801403062325\n",
      "  (18586, 5199)\t0.053755172633821714\n"
     ]
    }
   ],
   "source": [
    "vect_transformed_X_train = vect.transform(X_train)\n",
    "vect_transformed_X_test = vect.transform(X_test)\n",
    "print(vect_transformed_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18409,
     "status": "ok",
     "timestamp": 1615791583041,
     "user": {
      "displayName": "1CP Python",
      "photoUrl": "",
      "userId": "07491404253901504565"
     },
     "user_tz": -330
    },
    "id": "pyv9fs8fCEWu",
    "outputId": "61509f13-9155-4c24-ea2f-2da74f6c3b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 472622)\t0.23568377899173384\n",
      "  (0, 472621)\t0.2342160410749155\n",
      "  (0, 472158)\t0.12378677949394873\n",
      "  (0, 361562)\t0.3049516633126775\n",
      "  (0, 323962)\t0.13807927808671838\n",
      "  (0, 283651)\t0.23885330355470735\n",
      "  (0, 234925)\t0.3049516633126775\n",
      "  (0, 234924)\t0.27190248343369244\n",
      "  (0, 220992)\t0.24057278068060417\n",
      "  (0, 219593)\t0.096215025124363\n",
      "  (0, 202049)\t0.09634655020120825\n",
      "  (0, 202041)\t0.09516894012343538\n",
      "  (0, 193435)\t0.10807175420629142\n",
      "  (0, 116864)\t0.2597050144714942\n",
      "  (0, 91924)\t0.09421253451620981\n",
      "  (0, 60165)\t0.20934735068333196\n",
      "  (0, 59931)\t0.09320633169017124\n",
      "  (0, 26412)\t0.3049516633126775\n",
      "  (0, 26411)\t0.3049516633126775\n",
      "  (0, 26404)\t0.22897102742115066\n",
      "  (0, 24886)\t0.12422755895378258\n",
      "  (0, 7985)\t0.2155936950828051\n",
      "  (0, 7809)\t0.10915057477820543\n",
      "  (1, 502735)\t0.20287443094313376\n",
      "  (1, 501527)\t0.08531974146079438\n",
      "  :\t:\n",
      "  (6194, 321354)\t0.27960910576978854\n",
      "  (6194, 320453)\t0.24390276403857256\n",
      "  (6194, 191781)\t0.3369257313024119\n",
      "  (6194, 191590)\t0.18106625557396108\n",
      "  (6194, 113998)\t0.1942753997940223\n",
      "  (6194, 65956)\t0.3197519784998273\n",
      "  (6194, 62216)\t0.2600414742874117\n",
      "  (6194, 59931)\t0.22642437266962026\n",
      "  (6194, 39446)\t0.3704066440982546\n",
      "  (6195, 516848)\t0.30445060096695287\n",
      "  (6195, 516490)\t0.1811310656868486\n",
      "  (6195, 480790)\t0.3526812435280264\n",
      "  (6195, 480758)\t0.25964479233632\n",
      "  (6195, 398571)\t0.26344395084579497\n",
      "  (6195, 363258)\t0.07807502600016576\n",
      "  (6195, 298132)\t0.30445060096695287\n",
      "  (6195, 297889)\t0.17493474865446382\n",
      "  (6195, 293215)\t0.3526812435280264\n",
      "  (6195, 288979)\t0.10592312539148725\n",
      "  (6195, 197983)\t0.3385746848860695\n",
      "  (6195, 195433)\t0.11902489557645343\n",
      "  (6195, 161978)\t0.3526812435280264\n",
      "  (6195, 160144)\t0.1352716130623341\n",
      "  (6195, 8607)\t0.2467590977116173\n",
      "  (6195, 7809)\t0.12623430227073626\n"
     ]
    }
   ],
   "source": [
    "print(vect_transformed_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train a random forest classifier on the training data\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(vect_transformed_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7938992898644287\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "accuracy = rf.score(vect_transformed_X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tweet text and sentiment labels from the dataframe\n",
    "tweets = dataset['tweet'].values\n",
    "sentiments = dataset['hate_speech'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentiment labels as integers\n",
    "encoder = LabelEncoder()\n",
    "sentiments = encoder.fit_transform(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweet text and pad the sequences to a fixed length\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_tweets, test_tweets, train_sentiments, test_sentiments = train_test_split(padded_sequences, sentiments, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "310/310 [==============================] - 55s 171ms/step - loss: 0.4724 - accuracy: 0.7830 - val_loss: 0.3197 - val_accuracy: 0.7880\n",
      "Epoch 2/10\n",
      "310/310 [==============================] - 50s 160ms/step - loss: -0.0999 - accuracy: 0.7837 - val_loss: -0.0752 - val_accuracy: 0.7896\n",
      "Epoch 3/10\n",
      "310/310 [==============================] - 49s 157ms/step - loss: -0.9313 - accuracy: 0.7916 - val_loss: -0.2480 - val_accuracy: 0.7807\n",
      "Epoch 4/10\n",
      "310/310 [==============================] - 49s 159ms/step - loss: -2.0217 - accuracy: 0.7956 - val_loss: -0.2881 - val_accuracy: 0.7769\n",
      "Epoch 5/10\n",
      "310/310 [==============================] - 48s 156ms/step - loss: -3.0367 - accuracy: 0.7900 - val_loss: -0.8565 - val_accuracy: 0.7692\n",
      "Epoch 6/10\n",
      "310/310 [==============================] - 48s 156ms/step - loss: -4.9765 - accuracy: 0.7993 - val_loss: -1.3683 - val_accuracy: 0.7640\n",
      "Epoch 7/10\n",
      "310/310 [==============================] - 50s 161ms/step - loss: -6.6695 - accuracy: 0.7935 - val_loss: -1.1307 - val_accuracy: 0.7331\n",
      "Epoch 8/10\n",
      "310/310 [==============================] - 49s 157ms/step - loss: -7.7812 - accuracy: 0.7907 - val_loss: 1.5062 - val_accuracy: 0.7438\n",
      "Epoch 9/10\n",
      "310/310 [==============================] - 50s 161ms/step - loss: -9.7294 - accuracy: 0.7919 - val_loss: -1.8108 - val_accuracy: 0.7692\n",
      "Epoch 10/10\n",
      "310/310 [==============================] - 49s 158ms/step - loss: -12.3068 - accuracy: 0.7968 - val_loss: 4.3584 - val_accuracy: 0.6861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159c7e4ef70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build an LSTM model with an embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(train_tweets, train_sentiments, validation_data=(test_tweets, test_sentiments), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.358413219451904\n",
      "Test accuracy: 0.6861004829406738\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "test_loss, test_accuracy = model.evaluate(test_tweets, test_sentiments, verbose=0)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['positive', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# Use the model to make predictions on new data\n",
    "new_tweets = np.array(['I love this movie', 'I hate this food', 'I am feeling neutral today'])\n",
    "new_sequences = tokenizer.texts_to_sequences(new_tweets)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=100)\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "predicted_sentiments = ['positive' if p > 0.5 else 'negative' for p in predictions]\n",
    "print(\"Predictions:\", predicted_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(dataset['tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['tweet'])\n",
    "max_length = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxQvFvCruGIt"
   },
   "outputs": [],
   "source": [
    "sentiment_labels = np.where(dataset['hate_speech'] == 'positive', 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(padded_sequences))\n",
    "train_sequences = padded_sequences[:split_index]\n",
    "train_labels = sentiment_labels[:split_index]\n",
    "test_sequences = padded_sequences[split_index:]\n",
    "test_labels = sentiment_labels[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=embedding_dim, input_length=max_length),\n",
    "    LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 42s 64ms/step - loss: 0.0130 - accuracy: 0.9995\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 41s 66ms/step - loss: 3.6749e-05 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 40s 64ms/step - loss: 1.5181e-05 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 40s 65ms/step - loss: 8.4870e-06 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 41s 66ms/step - loss: 5.3220e-06 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 40s 64ms/step - loss: 3.5229e-06 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 40s 64ms/step - loss: 2.4183e-06 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 40s 65ms/step - loss: 1.6985e-06 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 41s 66ms/step - loss: 1.2061e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 40s 64ms/step - loss: 8.6678e-07 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159c65a1dc0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_sequences, train_labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/155 [==============================] - 2s 13ms/step - loss: 7.0625e-07 - accuracy: 1.0000\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_sequences, test_labels)\n",
    "print('Test accuracy:', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPnR2YTSyRce2U6SOhff58P",
   "collapsed_sections": [],
   "mount_file_id": "1DwlNtvSgEUrbv99kcLMdk28gERm3pxzG",
   "name": "hate_speech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
